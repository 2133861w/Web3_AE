{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>username</th>\n",
       "      <th>full_text</th>\n",
       "      <th>hashtag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5e8061e159de928bc9f9bbe8</td>\n",
       "      <td>tassawar rehman</td>\n",
       "      <td>This is wonderful news. \\n\\nBut we need a mech...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5e8061e159de928bc9f9bbe9</td>\n",
       "      <td>Aleks Kins</td>\n",
       "      <td>UK broadband providers lift data caps during c...</td>\n",
       "      <td>[technology]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5e8061e159de928bc9f9bbea</td>\n",
       "      <td>Jimmy Hunter üè≥Ô∏è‚Äçüåàüè¥Û†ÅßÛ†Å¢Û†Å≥Û†Å£Û†Å¥Û†Åøüá™üá∫ #StayAtHome</td>\n",
       "      <td>This should come as no surprise but Trump supp...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5e8061e159de928bc9f9bbeb</td>\n",
       "      <td>Alvaro Lagresa</td>\n",
       "      <td>Microsoft says it has \"divested its shareholdi...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5e8061e159de928bc9f9bbec</td>\n",
       "      <td>Ron Felice</td>\n",
       "      <td>A dashboard from the land before time  https:/...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>5e80b2de59de928bc9f9cb83</td>\n",
       "      <td>Liberal Momma</td>\n",
       "      <td>.@GovMikeDeWine: \"This is a matter of life and...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>5e80b2de59de928bc9f9cb84</td>\n",
       "      <td>üåäüíêAceysmommyüå∫üåä</td>\n",
       "      <td>.@GovMikeDeWine: \"This is a matter of life and...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>5e80b2de59de928bc9f9cb85</td>\n",
       "      <td>Bruce Edwards</td>\n",
       "      <td>Some can‚Äôt handle the truth.\\nMedical Expert W...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>5e80b2de59de928bc9f9cb86</td>\n",
       "      <td>Margaret Applin Designs</td>\n",
       "      <td>@IngrahamAngle This is called DUAL USE TECHNOL...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>5e80b2de59de928bc9f9cb87</td>\n",
       "      <td>Delia Payneüá∫üá∏</td>\n",
       "      <td>@bhweingarten Dr Brix is excellent.\\nI'm an en...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            id                                username  \\\n",
       "0     5e8061e159de928bc9f9bbe8                         tassawar rehman   \n",
       "1     5e8061e159de928bc9f9bbe9                              Aleks Kins   \n",
       "2     5e8061e159de928bc9f9bbea  Jimmy Hunter üè≥Ô∏è‚Äçüåàüè¥Û†ÅßÛ†Å¢Û†Å≥Û†Å£Û†Å¥Û†Åøüá™üá∫ #StayAtHome   \n",
       "3     5e8061e159de928bc9f9bbeb                          Alvaro Lagresa   \n",
       "4     5e8061e159de928bc9f9bbec                              Ron Felice   \n",
       "...                        ...                                     ...   \n",
       "3995  5e80b2de59de928bc9f9cb83                           Liberal Momma   \n",
       "3996  5e80b2de59de928bc9f9cb84                          üåäüíêAceysmommyüå∫üåä   \n",
       "3997  5e80b2de59de928bc9f9cb85                           Bruce Edwards   \n",
       "3998  5e80b2de59de928bc9f9cb86                 Margaret Applin Designs   \n",
       "3999  5e80b2de59de928bc9f9cb87                           Delia Payneüá∫üá∏   \n",
       "\n",
       "                                              full_text       hashtag  \n",
       "0     This is wonderful news. \\n\\nBut we need a mech...            []  \n",
       "1     UK broadband providers lift data caps during c...  [technology]  \n",
       "2     This should come as no surprise but Trump supp...            []  \n",
       "3     Microsoft says it has \"divested its shareholdi...            []  \n",
       "4     A dashboard from the land before time  https:/...            []  \n",
       "...                                                 ...           ...  \n",
       "3995  .@GovMikeDeWine: \"This is a matter of life and...            []  \n",
       "3996  .@GovMikeDeWine: \"This is a matter of life and...            []  \n",
       "3997  Some can‚Äôt handle the truth.\\nMedical Expert W...            []  \n",
       "3998  @IngrahamAngle This is called DUAL USE TECHNOL...            []  \n",
       "3999  @bhweingarten Dr Brix is excellent.\\nI'm an en...            []  \n",
       "\n",
       "[4000 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q2\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "\n",
    "__author__ = 'Ming Ho Wu'\n",
    "\n",
    "\"\"\"\n",
    "The clustering algorithm we are going to use is the K-mean clustering.\n",
    "That is because we learn this method in the Text as Data course,\n",
    "and a working implementation is provided in the Lab2 exercise.\n",
    "\"\"\"\n",
    "\n",
    "# Aim: extract the username, hashtag and full text of all the tweets\n",
    "#      store them into a panda dataframe\n",
    "# The reference for the solution is in the Lab2 Exercise in the Text as Data course.\n",
    "\n",
    "client = MongoClient()\n",
    "db = client[\"Q1b_DB\"]\n",
    "collection = db[\"Q1b_Collection\"]\n",
    "\n",
    "all_data = collection.find({})\n",
    "posts_tmp = list()\n",
    "collectAllHashtags = list()\n",
    "\n",
    "for data in all_data:\n",
    "    hashtagsList = list()\n",
    "    for hashtags in data[\"entities\"][\"hashtags\"]:\n",
    "            hashtagsList.append(hashtags[\"text\"])\n",
    "            collectAllHashtags.append(hashtags[\"text\"])\n",
    "    if \"retweeted_status\" in data:\n",
    "        posts_tmp.append((data[\"_id\"], data[\"user\"][\"name\"], data[\"retweeted_status\"][\"full_text\"], hashtagsList))\n",
    "    else:\n",
    "        posts_tmp.append((data[\"_id\"],data[\"user\"][\"name\"],data[\"full_text\"], hashtagsList))\n",
    "    \n",
    "    \n",
    "       \n",
    "labels = ['id','username','full_text','hashtag']\n",
    "refined_data = pd.DataFrame(posts_tmp,columns=labels)\n",
    "refined_data.head(4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "# Aim: Tokenize and normalize the text then vectorize it\n",
    "# Reference for solution is in the Lab 2 in Text as Data Course\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner'])\n",
    "\n",
    "def spacy_tokenize(string):\n",
    "  tokens = list()\n",
    "  doc = nlp(string)\n",
    "  for token in doc:\n",
    "    tokens.append(token)\n",
    "  return tokens\n",
    "\n",
    "\n",
    "def normalize(tokens):\n",
    "  normalized = list()\n",
    "  for token in tokens:\n",
    "    if (token.is_alpha or token.is_digit):\n",
    "      lemma = token.lemma_.lower().strip() if token.lemma_ != \"-PRON-\" else token.lower_\n",
    "      normalized.append(lemma)\n",
    "  return normalized\n",
    "\n",
    "\n",
    "def tokenize_normalize(string):\n",
    "  return normalize(spacy_tokenize(string))\n",
    "\n",
    "\n",
    "ngram_vectorizer = TfidfVectorizer(tokenizer=tokenize_normalize, stop_words='english',sublinear_tf=True, max_features=50000, ngram_range=(1,2))\n",
    "ngram_document_term_matrix = ngram_vectorizer.fit_transform(refined_data[\"full_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI\n",
      "Initialization complete\n",
      "Iteration  0, inertia 18475.633\n",
      "Iteration  1, inertia 9883.202\n",
      "Iteration  2, inertia 9467.125\n",
      "Iteration  3, inertia 9259.111\n",
      "Iteration  4, inertia 9213.149\n",
      "Iteration  5, inertia 9158.065\n",
      "Iteration  6, inertia 9072.000\n",
      "Iteration  7, inertia 9022.205\n",
      "Iteration  8, inertia 8999.756\n",
      "Iteration  9, inertia 8988.123\n",
      "Iteration 10, inertia 8988.086\n",
      "Converged at iteration 10: center shift 0.000000e+00 within tolerance 1.950940e-09\n",
      "Initialization complete\n",
      "Iteration  0, inertia 20213.622\n",
      "Iteration  1, inertia 9776.674\n",
      "Iteration  2, inertia 9354.613\n",
      "Iteration  3, inertia 9203.818\n",
      "Iteration  4, inertia 9155.208\n",
      "Iteration  5, inertia 9134.656\n",
      "Iteration  6, inertia 9130.158\n",
      "Iteration  7, inertia 9125.604\n",
      "Converged at iteration 7: center shift 0.000000e+00 within tolerance 1.950940e-09\n",
      "Initialization complete\n",
      "Iteration  0, inertia 20328.724\n",
      "Iteration  1, inertia 9963.340\n",
      "Iteration  2, inertia 9625.422\n",
      "Iteration  3, inertia 9530.665\n",
      "Iteration  4, inertia 9507.474\n",
      "Iteration  5, inertia 9469.921\n",
      "Iteration  6, inertia 9448.395\n",
      "Iteration  7, inertia 9440.729\n",
      "Iteration  8, inertia 9436.403\n",
      "Converged at iteration 8: center shift 0.000000e+00 within tolerance 1.950940e-09\n",
      "Initialization complete\n",
      "Iteration  0, inertia 9837.741\n",
      "Iteration  1, inertia 9748.998\n",
      "Iteration  2, inertia 9748.972\n",
      "Converged at iteration 2: center shift 0.000000e+00 within tolerance 1.950940e-09\n",
      "Initialization complete\n",
      "Iteration  0, inertia 19722.689\n",
      "Iteration  1, inertia 9873.662\n",
      "Iteration  2, inertia 9384.610\n",
      "Iteration  3, inertia 9178.843\n",
      "Iteration  4, inertia 9163.550\n",
      "Iteration  5, inertia 9149.840\n",
      "Iteration  6, inertia 9144.988\n",
      "Iteration  7, inertia 9137.330\n",
      "Iteration  8, inertia 9134.956\n",
      "Iteration  9, inertia 9130.730\n",
      "Iteration 10, inertia 9128.444\n",
      "Iteration 11, inertia 9128.436\n",
      "Converged at iteration 11: center shift 0.000000e+00 within tolerance 1.950940e-09\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='random', max_iter=300,\n",
       "       n_clusters=10, n_init=5, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aim: Perform K mean\n",
    "print(\"HI\")\n",
    "from sklearn.cluster import KMeans\n",
    "num_clusters = 10\n",
    "kmeans = KMeans(n_clusters=num_clusters, init='random', n_init=5, verbose=10)\n",
    "kmeans.fit(ngram_document_term_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      " correct trump\n",
      " trump target\n",
      " correct\n",
      " expert correct\n",
      " target far\n",
      " medical expert\n",
      " target\n",
      " expert\n",
      " medical\n",
      " far right\n",
      " far\n",
      " trump\n",
      " right\n",
      " right new\n",
      " new\n",
      " york times\n",
      " times\n",
      " new york\n",
      " york\n",
      " shameful attack\n",
      "---------------------\n",
      "Cluster 1:\n",
      " strongly word\n",
      " word press\n",
      " strongly\n",
      " remember strongly\n",
      " deployment mask\n",
      " limit deployment\n",
      " year remember\n",
      " morning blast\n",
      " cover 7\n",
      " press release\n",
      " 7 year\n",
      " release morning\n",
      " blast fda\n",
      " deployment\n",
      " blast\n",
      " cover\n",
      " 7\n",
      " release\n",
      " word\n",
      " press\n",
      "---------------------\n",
      "Cluster 2:\n",
      " sterilize\n",
      " mask\n",
      " day\n",
      " ohio\n",
      " base battelle\n",
      " despite ability\n",
      " ability sterilize\n",
      " day despite\n",
      " battelle sterilize\n",
      " authorize columbus\n",
      " columbus base\n",
      " ohio day\n",
      " day ohio\n",
      " fda authorize\n",
      " mask ohio\n",
      " authorize\n",
      " mask day\n",
      " sterilize surgical\n",
      " announce fda\n",
      " ability\n",
      "---------------------\n",
      "Cluster 3:\n",
      " ohio pioneers\n",
      " pioneers technology\n",
      " ppe radio\n",
      " pioneers\n",
      " coronavirus ohio\n",
      " reuse ppe\n",
      " radio\n",
      " clean reuse\n",
      " technology clean\n",
      " reuse\n",
      " clean\n",
      " ppe\n",
      " ohio\n",
      " coronavirus\n",
      " technology\n",
      " godfrey zoom\n",
      " gogravity\n",
      " globe\n",
      " globally mount\n",
      " globally let\n",
      "---------------------\n",
      "Cluster 4:\n",
      " ccp\n",
      " china\n",
      " china say\n",
      " chat group\n",
      " 1 forehead\n",
      " producer ccp\n",
      " thermometer producer\n",
      " dongguan guangdong\n",
      " say chat\n",
      " talk damage\n",
      " guangdong province\n",
      " damage hit\n",
      " way zhang\n",
      " china talk\n",
      " rule china\n",
      " hard ccpvirus\n",
      " ccp rule\n",
      " ccp way\n",
      " ccpvirus ccp\n",
      " guangdong\n",
      "---------------------\n",
      "Cluster 5:\n",
      " technology\n",
      " use\n",
      " coronavirus\n",
      " need\n",
      " fda\n",
      " say\n",
      " amp\n",
      " people\n",
      " make\n",
      " new\n",
      " day\n",
      " tech\n",
      " time\n",
      " work\n",
      " limit\n",
      " science\n",
      " today\n",
      " fight\n",
      " help\n",
      " country\n",
      "---------------------\n",
      "Cluster 6:\n",
      " like\n",
      " dewine exactly\n",
      " firebrand\n",
      " issue statement\n",
      " exactly firebrand\n",
      " really mean\n",
      " statement like\n",
      " firebrand issue\n",
      " know really\n",
      " like know\n",
      " exactly\n",
      " statement\n",
      " mean\n",
      " really\n",
      " issue\n",
      " know\n",
      " dewine\n",
      " father\n",
      " make devil\n",
      " devil work\n",
      "---------------------\n",
      "Cluster 7:\n",
      " example far\n",
      " supporter president\n",
      " trump regularly\n",
      " regularly vilify\n",
      " vilify oppose\n",
      " vilify\n",
      " regularly\n",
      " aim discrediting\n",
      " discrediting dr\n",
      " falsehood aim\n",
      " fauci example\n",
      " torrent falsehood\n",
      " discrediting\n",
      " oppose\n",
      " aim\n",
      " right supporter\n",
      " falsehood\n",
      " torrent\n",
      " president trump\n",
      " example\n",
      "---------------------\n",
      "Cluster 8:\n",
      " time\n",
      " access\n",
      " work\n",
      " bad information\n",
      " information earth\n",
      " insanely\n",
      " insanely mad\n",
      " earth insanely\n",
      " allow bad\n",
      " home access\n",
      " home\n",
      " stick home\n",
      " mad\n",
      " stelter cnn\n",
      " join brian\n",
      " 11 noon\n",
      " noon\n",
      " cnn reliable\n",
      " reliable source\n",
      " today 11\n",
      "---------------------\n",
      "Cluster 9:\n",
      " late\n",
      " thank\n",
      " daily\n",
      " technology\n",
      " daily thank\n",
      " god technology\n",
      " god\n",
      " love\n",
      " conservative\n",
      " thank god\n",
      " thank technology\n",
      " late technology\n",
      " rt\n",
      " use technology\n",
      " job\n",
      " check\n",
      " link bio\n",
      " technology daily\n",
      " robert half\n",
      " half technology\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "# Aim: Extract the top 20 important entities/ concepts for each group\n",
    "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = ngram_vectorizer.get_feature_names()\n",
    "\n",
    "\n",
    "for i in range(num_clusters):\n",
    "  print(\"Cluster %d:\" % i)\n",
    "  for ind in order_centroids[i, :20]:\n",
    "    print(' %s' % terms[ind])  \n",
    "  print(\"---------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Security Testing', 'Cyber Security News', 'Cyber Security Feed', 'HubofMachineLearning', 'CyberSecurityBot ü§ñ', 'Virtual Consultnts', 'Chidambara .ML.', 'Ochieng Scott', 'iC0dE', 'Milo Camacho']\n"
     ]
    }
   ],
   "source": [
    "# find the top 10 usernames\n",
    "\n",
    "\n",
    "\n",
    "top_10_frequency = 10\n",
    "top10_usernames = refined_data['username'].value_counts()[:top_10_frequency].index.tolist()\n",
    "\n",
    "print(top10_usernames)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('technology', 384), ('COVID19', 182), ('China', 172), ('CCPVirus', 166), ('CCP', 165), ('Ghilli', 142), ('Technology', 118), ('tech', 106), ('AI', 94), ('coronavirus', 69)]\n"
     ]
    }
   ],
   "source": [
    "# find the top 10 hashtags\n",
    "\n",
    "top_10_frequency = 10\n",
    "\n",
    "\n",
    "# Reference for solution:\n",
    "# https://www.geeksforgeeks.org/python-find-most-frequent-element-in-a-list/\n",
    "  \n",
    "from collections import Counter \n",
    "  \n",
    "def most_frequent(List): \n",
    "    occurence_count = Counter(List) \n",
    "    return occurence_count.most_common(10)\n",
    "    \n",
    "print(most_frequent(collectAllHashtags)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
